{
 "metadata": {
  "name": "NLP Class Week1-2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Useful forum questions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. [Does M in perplexity formula Contain Stopwords](https://class.coursera.org/nlangp-001/forum/thread?thread_id=292)\n",
      "1. [Why not learn P(Y|X) from the training corpus directly](https://class.coursera.org/nlangp-001/forum/thread?thread_id=263)"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Noisy Channel Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have some original signal, say a word, and it goes through some kind of channel, a noisy channel, and you get a noisy word. The noisy channel a phone for speech, or a spelling mistake in the case of text.\n",
      "\n",
      "So you model how the noisy channel works, run all possible words through it, and pick which one looks most like the noisy word.\n",
      "\n",
      "You see this modeled often using Bayes Theorem\n",
      "\n",
      "\n",
      "$$P(\\textbf{w}|\\textbf{x}) = \\frac{P(\\textbf{x}|\\textbf{w}) * P(\\textbf{w})}{P(\\textbf{x})}$$\n",
      "\n",
      "\n",
      "where the first term on top on the right is conditional probability is the probability of getting x, the distorted word, given you started with w, the actual word. This portion of the equation is called the channel model. The other second term on top, is just the probability of w occurring in general and is called the prior. The denominator can be ignored since it is the same against all comparisons.\n",
      "\n",
      "This is used in generative models (I think).\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generative (Joint) Models Vs Discriminative Models"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Examples of Generative (Joint) Models Include:\n",
      "https://class.coursera.org/nlp/lecture/38\n",
      "\n",
      "https://class.coursera.org/nlangp-001/lecture/83 @7:20\n",
      "\n",
      "1. Naive Bayes\n",
      "1. N-gram Language Models\n",
      "1. Probabilistic Context-free Grammars\n",
      "1. Hidden Markov Models\n",
      "\n",
      "###Examples of Discriminative (conditional) Models:\n",
      "1. Logist regression\n",
      "1. conditional loglinear or maximum entropy models\n",
      "1. conditional random fields\n",
      "1. SVMs\n",
      "\n",
      "###Explanation of differences\n",
      "https://class.coursera.org/nlp/lecture/38 @ 3:40 is a nice explananation\n",
      "\n",
      "We have some data ({d,c}) pairs, where d is an observation, and c is a hidden class or label.\n",
      "####Joint\n",
      "Joint models place probabilities over both the observed data and the hidden stuff- P(c,d)\n",
      "They generate the observed data from the hidden stuff.\n",
      "####Conditional\n",
      "Take the data as a given, and put a probability over the hidden structure given the data- P(c|d)\n",
      "They can be very prone to over fitting on training data.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO what the bayes videos in NLP1 course and the HMM in the second one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}