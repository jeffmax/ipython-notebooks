{
 "metadata": {
  "name": "Naive Bayes Classifier"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "Background"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "A Naive Bayes Classifier gets its name from the fact that it consider each occurence of a feature in a document an independent event. In reality, it is not true that the occurrence of the words \"free\" and \"gold\" are completely unrelated events in a spam e-mail, but for the purposes of simplicity, a naive bayes classifier uses this assumption."
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": "Useful References"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "1. https://github.com/jergason/credulous\n1. https://www.bionicspirit.com/blog/2012/02/09/howto-build-naive-bayes-classifier.html\n1. http://www.dusbabek.org/~garyd/bayes/bayes.js\n1. http://en.wikipedia.org/wiki/Bayesian_spam_filtering#Other_expression_of_the_formula_for_combining_individual_probabilities"
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "How It Works"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "I learned the basics behind this type of classifier from the book Programming Collective Intelligence. The section is a bit confusing, and at times I was convinced it was wrong, but I think I finally understand it. I will discuss some of the confusion I had as I progress."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The end-goal is to produce P(Category|Document). This is read as \"the probability of a Category given a Document\". We have a Document and we would like to know what category it belongs in. To do this, we will have to use Bayes Rule. This part I won't really discuss because I am currently still taking it as magic, but the concept is that it allows us obtain P(A|B) from P(B|A) and the two probabilities P(A) and P(B). The reason we need this is that with the approach described it is easy to calculate P(Document|Category), but we want the opposite. So, Bayes Rule:"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "$\\begin{equation}\n \\Pr(A|B)=\\frac{\\Pr(B|A)\\Pr(A)}{\\Pr(B|A)\\Pr(A)+\\Pr(B|\\neg A)\\Pr(\\neg A)}\n\\end{equation}$\n\nThe denominator on the right-side is actually just P(B) (but this form will be interesting later), so you can write it like this\n\n\\begin{equation}\n \\Pr(A|B)=\\frac{\\Pr(B|A)\\Pr(A)}{\\Pr(B)}\n\\end{equation}\n\nor more concretely for this task\n\n\\begin{equation}\n \\Pr(Category|Document)=\\frac{\\Pr(Document|Category)\\Pr(Category)}{\\Pr(Document)}\n\\end{equation}\n"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "The book makes the simplication here that we actually don't need to know the Pr(Document) because in reality, we are going to take one document and run this equation for all the categories we are searching through. The Document is a constant, so dividing by it will not change anything. It is important to point out that once you remove the denominator here you are no longer getting the probability of Category given Document, but you are getting some value that can be used to compare against the other values, and that is all we need to solve this problem.  If you actually want to get the probability you have to leave the denominator in. This is not a big deal, but it can change the way this problem is solved and/or look very different. An example of this is the fourth link under useful references, which solves the same problem, but leaves the denominator in while also avoiding floating point underflow. I will return to that later.\n\nThe assumption here is that we have trained the classifier on a corpus of sentences with known categories. We now see a new unseen document and want to categorize it. So we need two things:\n\n$\\Pr(Document|Category)$ \n\nand \n\n$\\Pr(Category)$\n\nLets start with $\\Pr(Category)$\n\n\nThis one is simple. We take the number of documents in the category we are looking for divided by the total number of documents. Now for \n\n$\\Pr(Document|Category)$ \n\nWe do this by looking at all the features in the document in question. In this case a feature will just be a word. Having previously trained a series of documents, we know how many times a given feature appeared in a document labeled with a given category. For example, we might know that the word \"sauce\" appeared in 20 documents labeled as \"recipes\" and 1 document labeled as \"news story\". So the book explains that we extract the features of the document with a function called getFeatures, and store these in an array. __Major Confusion Point__: this function returns the _unique_ features. So we take the number of times this feature appeared in a document with our category and divide it by the total number of documents in this category. Because the getFeatures function returns only one occurence of each feature, we know that the number of times this feature appeared in a document labeled with our category will not be greater than the number of documents in our category (which would give us a probability > 1).\n\nSidenote: Why not take the number of times this feature appeared in a document with our category and divide by the number of documents this feature appeared in? What is the difference? The key difference is that the above calculation takes into account the total number of documents in the category. Reminder: we are looking for $$Pr(Document|Category)$$, or Document _given_ category). The second proposed probability only takes into account the documents our feature occurs in, and while on the surface dividing the number of times we found our feature in a document labeled with our category by the number of times we found our feature might sound right, it is really the probability of finding a document with our feature labeled with our category in a pile of documents containing our feature. That is not what we want.\n\nNow the book does not just use this probability straight out. It needs to apply some kind of correction or smoothing for cases where we find a feature we have never seen. Basically it does this by always adding in a document that has a .5 percent chance of having our feature. The formula ends up being a weighted average:\n\n$$\\frac{(1*.5)+total documents in category*unsmoothed probability}{total documents in category+1}$$\n\nThis is basically saying: add an extra document in with a .5 probability of being in the category to the unsmoothed probability times the total documents in the category (which by the way, would be the number of expected items in the category with our feature since that is what the unsmoothed probability is, the likelihood a feature appears in a given category) and divide by the new total number of documents in the category, which is 1 more than it used to be.\n\nSo this gives us the probability that a given feature is in a document in our category. To get the full document probability we take all the features in the document and multiply them out. This is how you would calculate any occurence that is the result of a series of independent events, which is the assumption we make with the Naive Bayes classifier.\n\nOne thing that is not mentioned in the book is the fact that depending on the number of features and their probabilities, you could end up getting such a small number that it would result in floating point underflow in most computer languages. This is an implementation detail, and while not part of the conceptual math behind the classifier, could still result in the classifier misbehaving. There is more than one way to prevent this, but the popular method, especially when using the simplifying assumption made in the book that we do not need the denominator (if you leave it in there is some manipulation you can do to get the full probability without risking underflow rather than what we will have, which is just a number we can use to compare to the calculations we do for the other documents). Instead of multiplying by all the feature probabilities, add the natural log of each probability.\n\nSo, we now take this number, multiply it by $\\Pr(C)$ (or if you went the log route, add it to the natural log of $\\Pr(C)$) and you have the ranking assigned to this category. Now do this for all the possible categories and pick the one that results in the highest number.\n\n\n\n\n"
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "More Implementation: Thresholding"
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": "More More Implementation: Removing stops words and stemming"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": "Alternate methods (particulary if you need a real probablity)"
    }
   ],
   "metadata": {}
  }
 ]
}